{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias is set to False, since it would be cancelled out by the batchnorm anyway. Batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],\n",
    "            ):\n",
    "        super(UNET,self).__init__()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Good idea to have an input divisible by 2^4 (4 layers in depth) to avoid problems with inputs not having the same height and width\n",
    "\n",
    "        # Down part of UNET\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Up part of UNET\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    # feature*2 gets saved to feature\n",
    "                    feature*2, feature, kernel_size=2, stride=2,\n",
    "                )\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "\n",
    "        # features[-1] gets 512 that is the last of our features from the UNET\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1,)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        # Order is important: first with highest resolution and last with lowest\n",
    "        for downs in self.downs:\n",
    "            x = downs(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        # Reversing skip_connections\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        # Reason for step = 2, is because we want to double conv for each step up\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            # idx//2 to compensate for step = 2\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "\n",
    "            # Resizes if shape is not the same\n",
    "            if x.shape != skip_connection.shape:\n",
    "                # Takes out only the height and the width, so skipping batch-size and number of channels\n",
    "                x = FT.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "\n",
    "        return self.final_conv(x)\n",
    "    \n",
    "    def test():\n",
    "        # Batch size of 3, input 1 channel, 160 for features, which is perfectly divisible\n",
    "        x = torch.random((3, 1, 160, 160))\n",
    "        model = UNET(in_channels=1, out_channels=1)\n",
    "        preds = model(x)\n",
    "        # Make sure that input is the exact same shape as output\n",
    "        print(preds.shape)\n",
    "        print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-contrib-python\n",
    "%pip install roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "import cv2\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = Roboflow(api_key=\"e6vqYw2hVrLBzvqxJoQc\")\n",
    "project = rf.workspace().project(\"taco-trash-annotations-in-context\")\n",
    "model = project.version(16).model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer on a local image\n",
    "print(model.predict('best_pic_0.png').json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(\"best_pic_0.png\").save(\"seg_prediction.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cropping img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('best_pic_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_data = model.predict('best_pic_0.png').json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, prediction in enumerate(predictions_data['predictions']):\n",
    "    x, y, width, height = int(prediction['x']), int(prediction['y']), int(prediction['width']), int(prediction['height'])\n",
    "    \n",
    "    # Crop the area based on bounding box coordinates\n",
    "    roi_x = int(prediction['x'] - prediction['width'] / 2)\n",
    "    roi_y = int(prediction['y'] - prediction['height'] / 2)\n",
    "    roi_width = int(prediction['width'])\n",
    "    roi_height = int(prediction['height'])\n",
    "\n",
    "    cropped_area = image[roi_y:roi_y+roi_height, roi_x:roi_x+roi_width]\n",
    "    \n",
    "    # Save the cropped area\n",
    "    cv2.imwrite(f'cropped_object_{index}.png', cropped_area)\n",
    "    \n",
    "    # Draw circle on the cropped area, adjust the x, y relative to the cropped area\n",
    "    relative_x = x - roi_x\n",
    "    relative_y = y - roi_y\n",
    "    cv2.circle(cropped_area, (relative_x, relative_y), 5, (0, 0, 255), -1)  # Radius=5, Color=Green, Thickness=-1 (filled)\n",
    "    \n",
    "    # Draw coordinates above the circle\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    text = f\"({relative_x}, {relative_y})\"\n",
    "    text_size = cv2.getTextSize(text, font, 0.5, 1)[0]\n",
    "    text_x = relative_x - text_size[0] // 2\n",
    "    text_y = relative_y - 10  # 10 pixels above the circle\n",
    "    cv2.putText(cropped_area, text, (text_x, text_y), font, 0.5, (0, 0, 255), 1)\n",
    "\n",
    "    \n",
    "    # Save the cropped area with the circle\n",
    "    cv2.imwrite(f'cropped_object_with_circle_{index}.png', cropped_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image = cv2.imread('cropped_object_with_circle_1.png')\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from IPython.display import Image, display\n",
    "\n",
    " \n",
    "url = 'https://sickcv-prediction.cognitiveservices.azure.com/customvision/v3.0/Prediction/0099a351-a017-4271-8ae9-fac952b93363/detect/iterations/trashRecognizer/image'\n",
    "\n",
    "# Load the original image\n",
    "original_image = cv2.imread('best_pic_0.png') \n",
    "\n",
    " # ('',('best_pic_0.png',open('/Volumes/Untitled/rgb_0.png','rb'),'image/png'))\n",
    "payload = {}\n",
    "files=[\n",
    "  #  ('',('best_pic_0',open('C:\\\\Users\\\\Simons Lenovo\\\\Desktop\\\\Hackathon_2023\\\\Test_data\\\\best_pic_0','rb'),'image\\\\png'))\n",
    "  ('',('best_pic_0.png',open('best_pic_0.png','rb'),'image/png'))\n",
    "]\n",
    "headers = {\n",
    "  'Prediction-Key': '3fc4ab02162c46379ebce6444363943f'\n",
    "}\n",
    " \n",
    "response = requests.request(\"POST\", url, headers=headers, data=payload, files=files)\n",
    "print(response.text)\n",
    "\n",
    "# Assuming 'response' contains the JSON response\n",
    "response_json = json.loads(response.text)\n",
    "\n",
    "# Set the confidence threshold (adjust as needed)\n",
    "confidence_threshold = 0.8\n",
    "\n",
    "# Iterate through detected objects in the response\n",
    "for prediction in response_json['predictions']:\n",
    "    label = prediction['tagName']\n",
    "    confidence = prediction['probability']\n",
    "\n",
    "    # Check if the confidence score exceeds the threshold\n",
    "    if confidence >= confidence_threshold:\n",
    "        # Get bounding box coordinates\n",
    "        left = int(prediction['boundingBox']['left'] * original_image.shape[1])\n",
    "        top = int(prediction['boundingBox']['top'] * original_image.shape[0])\n",
    "        width = int(prediction['boundingBox']['width'] * original_image.shape[1])\n",
    "        height = int(prediction['boundingBox']['height'] * original_image.shape[0])\n",
    "\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(original_image, (left, top), (left + width, top + height), (0, 255, 0), 2)\n",
    "\n",
    "        # Display label and confidence\n",
    "        label_text = f\"{label} ({confidence:.2f})\"\n",
    "        cv2.putText(original_image, label_text, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "# Save the image with bounding boxes and labels to a file\n",
    "output_image_path = 'output_image.jpg'\n",
    "cv2.imwrite(output_image_path, original_image)\n",
    "\n",
    "# Display the saved image in the Jupyter Notebook\n",
    "display(Image(filename=output_image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
